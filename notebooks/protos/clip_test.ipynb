{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49681f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/creux/Documents/AI/VIFagent/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import open_clip\n",
    "import torch\n",
    "\n",
    "\n",
    "clip_model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    \"ViT-B-32\", pretrained=\"laion2b_s34b_b79k\"\n",
    ")\n",
    "device = torch.device(\"cpu\")\n",
    "clip_model = clip_model.to(device)\n",
    "clip_model.eval()\n",
    "clip_tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f18a96dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sims(a_labels,b_labels):\n",
    "\n",
    "    clip_encoded_a_labels = clip_model.encode_text(clip_tokenizer(a_labels))\n",
    "    clip_encoded_b_labels = clip_model.encode_text(clip_tokenizer(b_labels))\n",
    "    with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "        text_sims = (clip_encoded_a_labels @ clip_encoded_b_labels.T)\n",
    "        \n",
    "    for alabel,sims in zip(a_labels,text_sims):\n",
    "        print(f\"similarity {alabel}\")   \n",
    "        for blabel,sim in zip(b_labels,sims):\n",
    "            print(f\"    -{blabel}: {sim}\")\n",
    "    return text_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b243d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity left eye\n",
      "    -left pupil: 58.51355743408203\n",
      "    -right pupil: 51.71540832519531\n",
      "similarity right eye\n",
      "    -left pupil: 54.226497650146484\n",
      "    -right pupil: 61.25897216796875\n",
      "similarity mouth\n",
      "    -left pupil: 38.34929656982422\n",
      "    -right pupil: 41.58185577392578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[58.5136, 51.7154],\n",
       "        [54.2265, 61.2590],\n",
       "        [38.3493, 41.5819]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_labels = [\"left eye\",\"right eye\",\"mouth\"]\n",
    "b_labels = [\"left pupil\",\"right pupil\"]\n",
    "\n",
    "get_sims(a_labels,b_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfa0bef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_cor_labels(a_labels, b_labels):\n",
    "\n",
    "    if len(b_labels) > len(a_labels):\n",
    "        tmp = b_labels\n",
    "        b_labels = a_labels\n",
    "        a_labels = tmp\n",
    "\n",
    "    clip_encoded_a_labels = clip_model.encode_text(clip_tokenizer(a_labels))\n",
    "    clip_encoded_b_labels = clip_model.encode_text(clip_tokenizer(b_labels))\n",
    "    with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "        text_sims = clip_encoded_a_labels @ clip_encoded_b_labels.T\n",
    "\n",
    "    text_sims = sorted(zip(text_sims, a_labels), key=lambda d: max(d[0]), reverse=True)\n",
    "    label_correspondance = {}\n",
    "    used_simlabel_indexes = []\n",
    "    b_labels = np.array(b_labels)\n",
    "\n",
    "    for sims, alabel in text_sims:\n",
    "        m_sims = sims\n",
    "        if len(used_simlabel_indexes) != 0:\n",
    "            mask = torch.ones(sims.size(0), dtype=torch.bool)\n",
    "            mask[used_simlabel_indexes] = False\n",
    "            m_sims = sims[mask]\n",
    "            if len(m_sims) == 0:\n",
    "                break\n",
    "            b_labels = np.delete(b_labels, used_simlabel_indexes)\n",
    "\n",
    "        index_max = torch.argmax(m_sims)\n",
    "        label_correspondance[alabel] = b_labels[index_max]\n",
    "        used_simlabel_indexes.append(index_max.item())\n",
    "    return label_correspondance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5077ab18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'right eye': np.str_('right pupil'), 'left eye': np.str_('left pupil')}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_labels = [\"left eye\",\"right eye\",\"mouth\"]\n",
    "b_labels = [\"left pupil\",\"right pupil\"]\n",
    "\n",
    "get_cor_labels(a_labels,b_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd671cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VIFagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
